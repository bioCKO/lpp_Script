
from collections import defaultdict
import logging
import os

from pbcore.io import GffReader
from pbcoretools.pbvalidate import fasta, dataset
from pbcommand.models.common import FileTypes

from pbsmrtpipe.testkit.base import monkey_patch
from pbsmrtpipe.testkit.validators2 import (ValidateJsonReport, ValidateJson,
                                            DataStoreFileValidator)

from pysiv2.custom.base import TestBase

log = logging.getLogger(__name__)

QUICK_MODE = bool(os.environ.get("SMRT_TEST_QUICK_MODE", False))


class InvalidateDataStoreFile(ValueError):
    pass


def raise_if_invalid(func):
    # deco to call validate funcs that return errors, metrics
    # will raise if there's an error, or return true
    def wrapper(path):
        errors, metrics = func(path)
        if errors:
            for e in errors:
                log.error("Invalid file {p} {e}".format(e=e, p=path))
            # FIXME. Better error message. This might be too verbose
            emsg = "\n".join([str(e) for e in errors])
            log.error(emsg)
            raise InvalidateDataStoreFile(emsg)
        return True
    return wrapper


@raise_if_invalid
def _validate_fasta(path):
    return fasta.validate_fasta(file_name=path)


@raise_if_invalid
def _validate_alignment_dataset(path):
    return dataset.validate_dataset(
        file_name=path,
        dataset_type="AlignmentSet",
        # contents="SUBREAD",
        quick=QUICK_MODE,
        #max_errors=1,
        max_records=5000,
        aligned=True,
        strict=True,
        validate_index=True)


@raise_if_invalid
def _validate_subread_dataset(path):
    return dataset.validate_dataset(
        file_name=path,
        dataset_type="SubreadSet",
        contents="SUBREAD",  # redundant, shown for clarity
        max_errors=1,
        quick=QUICK_MODE,
        aligned=False,
        strict=True,
        validate_index=True)


@raise_if_invalid
def _validate_hdfsubread_dataset(path):
    return dataset.validate_dataset(
        file_name=path,
        dataset_type="HdfSubreadSet",
        strict=True)


@raise_if_invalid
def _validate_ccs_dataset(path):
    return dataset.validate_dataset(
        file_name=path,
        dataset_type="ConsensusReadSet",
        contents="CCS",  # redundant, shown for clarity
        max_errors=1,
        quick=QUICK_MODE,
        aligned=False,
        validate_index=True,
        strict=True)


@raise_if_invalid
def _validate_ccs_alignment_dataset(path):
    return dataset.validate_dataset(
        file_name=path,
        dataset_type="ConsensusAlignmentSet",
        contents="CCS",  # redundant, shown for clarity
        quick=QUICK_MODE,
        max_errors=1,
        aligned=True,
        validate_index=True,
        strict=True)


@raise_if_invalid
def _validate_contigset(path):
    return dataset.validate_dataset(
        file_name=path,
        dataset_type="ContigSet",
        quick=QUICK_MODE,
        max_errors=1,
        strict=True)


ValidateFasta = DataStoreFileValidator(
    FileTypes.FASTA.file_type_id, _validate_fasta)
ValidateContigSet = DataStoreFileValidator(
    FileTypes.DS_CONTIG.file_type_id, _validate_contigset)
ValidateAlignmentSet = DataStoreFileValidator(
    FileTypes.DS_ALIGN.file_type_id, _validate_alignment_dataset)
ValidateSubreadDataSet = DataStoreFileValidator(
    FileTypes.DS_SUBREADS.file_type_id, _validate_subread_dataset)
ValidateHdfSubreadDataSet = DataStoreFileValidator(
    FileTypes.DS_SUBREADS_H5.file_type_id, _validate_hdfsubread_dataset)
ValidateConsensusReadDataSet = DataStoreFileValidator(
    FileTypes.DS_CCS.file_type_id, _validate_ccs_dataset)
ValidateConsensusAlignmentDataSet = DataStoreFileValidator(
    FileTypes.DS_ALIGN_CCS.file_type_id, _validate_ccs_alignment_dataset)


@monkey_patch
class TestValidDataStoreFiles(TestBase):

    """
    Load Files by type in datastore and validate them using pbvalidate
    """

    DATASTORE_FILE_VALIDATORS = (ValidateFasta,
                                 ValidateJson,
                                 ValidateContigSet,
                                 ValidateJsonReport,
                                 ValidateAlignmentSet,
                                 ValidateSubreadDataSet,
                                 ValidateHdfSubreadDataSet,
                                 ValidateConsensusReadDataSet,
                                 ValidateConsensusAlignmentDataSet)


class TestGFF(TestBase):

    """
    Validate the contents of GFF files
    """

    @classmethod
    def setUpClass(cls):
        super(TestGFF, cls).setUpClass()
        cls.gff_files = []
        for file_id, file_info in cls.datastore.get_file_dict().iteritems():
            if file_info.file_type_id == FileTypes.GFF.file_type_id:
                cls.gff_files.append(file_info.path)

    def test_gff_seqid_is_fasta_identifier(self):
        """
        Check that GFF files use only the identifier part of FASTA headers,
        no spaces allowed - see ticket 28667
        """
        for gff_file in self.gff_files:
            with GffReader(gff_file) as r:
                for rec in r:
                    self.assertTrue(not " " in rec.seqid,
                                    "seqid contains spaces:\n%s\n(file: %s)" %
                                    (str(rec), gff_file))

    def test_gff_file_headers(self):
        """
        Check that every GFF file contains headers.
        """
        for gff_file in self.gff_files:
            with GffReader(gff_file) as r:
                self.assertTrue(len(r.headers) > 0,
                                "No headers in %s" % gff_file)

    def test_gff_sort_order(self):
        """
        Check that records in all GFF output files are in sorted order
        (verification for bug 27785).
        """
        for gff_file in self.gff_files:
            with GffReader(gff_file) as gff:
                last_rec = None
                for rec in gff:
                    if last_rec is not None and rec.seqid == last_rec.seqid:
                        self.assertTrue(rec.start >= last_rec.start,
                                        "Records occur out of order:\n{l}\n{r}".format(r=rec, l=last_rec))
                    last_rec = rec
